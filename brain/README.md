# 🧠 Brain (대뇌 - 판단과 사고)

## 📖 개요 (Overview)
이 폴더는 로봇의 **'두뇌'**에 해당합니다. 
사람이 눈과 귀로 정보를 받아들이고 생각을 하듯이, 이 시스템도 외부 정보를 바탕으로 **무엇을 해야 할지 논리적으로 판단**하는 곳입니다.
최신 AI 기술인 **LLM(거대 언어 모델)**을 사용하여 사람의 말을 이해하고 상황에 맞는 대답이나 행동을 결정합니다.

---

## 📂 파일 구조 및 상세 설명 (Structure & Files)

### 1. `logic_brain.py` (핵심 로직)
- **역할**: 뇌의 **지휘관**입니다.
- **기능**:
    - **LangChain(랭체인)**이라는 도구를 사용하여 AI 모델을 초기화합니다.
    - 사용자가 한 말과 현재 로봇이 보고 있는 상황(Context)을 합쳐서 AI에게 전달합니다.
    - AI가 "인사를 해"라고 결정하면, 그 결정을 시스템의 다른 부서(레이어)로 전파합니다.
- **코드 비유**: 
    - "귀(Sensor)"로 들은 말과 "눈(Vision)"으로 본 것을 섞어서 "생각(LLM)"을 한 뒤, "입(Speaker)"이나 "몸(Robot)"에게 명령을 내리는 과정입니다.

### 2. `prompts.py` (지도 지침)
- **역할**: AI에게 주는 **'행동 강령'** 또는 **'가이드라인'**입니다.
- **기능**: 
    - "너는 친절한 로봇이야", "위험한 행동은 하지 마"와 같은 성격과 규칙이 적혀 있습니다.
    - AI는 이 지침을 가장 최우선으로 따릅니다.

### 3. `emotion_updater.py` (감정 변화)
- **역할**: 생각의 결과에 따라 **'기분'**을 바꾸는 파일입니다.
- **기능**:
    - 예를 들어, 사용자가 칭찬을 하면 '기쁨' 수치를 올리고, 욕을 하면 '슬픔' 수치를 올리라고 판단합니다.
    - 실제 감정을 느끼는 것은 아니고, 수학적인 수치(벡터)를 조절하는 역할을 합니다.

### 4. `tools/` (폴더)
- **역할**: 뇌가 사용할 수 있는 **'도구 상자'**입니다.
- **내용**:
    - `vision_detect.py`: "지금 앞에 뭐가 있어?"라고 물어볼 때 사용하는 눈 도구.
    - `grasp_object.py`: "저거 잡아"라고 할 때 사용하는 손 도구.
    - AI는 필요할 때 이 도구 상자에서 적절한 도구를 꺼내 사용합니다.

---

## ⚙️ 작동 원리 (Logic Flow)

1. **입력**: 사용자가 채팅으로 명령을 내립니다 (예: "오리를 잡아줘").
2. **인지(Perception)**: `logic_brain.py`가 현재 시야에 '오리'가 있는지 확인합니다.
3. **사고(Thinking)**: AI가 `prompts.py`의 지침에 따라 생각합니다.
    - "사용자가 오리를 잡으라고 했네. 내 도구 상자에 잡는 도구(`grasp_object`)가 있나? 있네. 그럼 그걸 써야겠다."
4. **결정(Decision)**: AI가 "오리 잡기(PickUp)" 행동을 하겠다고 결정합니다.
5. **전파(Broadcast)**: 이 결정이 `pipeline.py`를 타고 전신으로 퍼져나가, 실제로 로봇 팔이 움직이게 됩니다.

---

## 🔗 상속 및 관계 (Relationships)
- **상위(Upstream)**: `Sensor` (정보를 받음)
- **하위(Downstream)**: `Strategy`, `Expression` (판단을 내려보냄)
- **의존성(Dependency)**: `state/system_state.py` (현재 상태를 읽어옴), `tools/` (도구를 사용함)
